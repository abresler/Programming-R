{
    "contents" : "install.packages('twitteR')\nlibrary(twitteR)\nlibrary(ggplot2)\n\n\n# 搜索某个关键词的推文（关于moyan的发推用户和内容） --------------------------------------------------------------\n\n# 观察哪些用户使用了这个关键词\nmoyanTweets <- searchTwitter('#moyan', n = 1000)\nmoyandf <- twListToDF(rdmTweets)\n#下一步提取推文用户名并制成频数表\ncounts <- table(moyandf$screenName)\ncc <- data.frame(subset(counts, counts>5))\nnames(cc) <- 'value'\ncc <- data.frame(value = cc$value,name = rownames(cc))\nnewname <- with(cc, reorder(name, value))\ndata <- data.frame(cc, newname)\n#载入ggplot2包，绘制条形图。\np <- ggplot(data,aes(y=value,x=newname))\np+geom_bar(stat='identity',aes(fill=value))+coord_flip()\n\nggsave('1.png')\n# simopieranni是驻北京的一位意大利记者 SCMP_news南华早报\n\n# 用词云研究推文中的内容\nlibrary(tm)\nlibrary(wordcloud)\n#为了回避一些推文中的网址等符号，用gsub加以去除\nmoyantext <- moyandf$text\npattern <- \"http[^ ]+|RT |@[^ ]+\"\ntext <- gsub(pattern, \"\", moyantext)\n# 再用tm包建立文本库和词频矩阵\ntw.corpus <- Corpus(VectorSource(text))\ntw.corpus <- tm_map(tw.corpus, stripWhitespace)\ntw.corpus <- tm_map(tw.corpus, removePunctuation)\ntw.corpus <- tm_map(tw.corpus,tolower)\ntw.corpus <- tm_map(tw.corpus,removeWords,stopwords('english'))\n\ndoc.matrix <- TermDocumentMatrix(tw.corpus,control = list(minWordLength = 1))\ndm <- as.matrix(doc.matrix)\nv <- sort(rowSums(dm),decreasing=T)\nd <- data.frame(word=names(v),freq=v)\n\n#去除moyan和nobel这两个词后，生成最后的词云\nworddata <- d[3:50,]\nworddata$word <- factor(worddata$word)\nmycolors <- colorRampPalette(c(\"white\",\"red4\"))(200)\nwc <-wordcloud(worddata$word,worddata$freq,min.freq=13,colors=mycolors[100:200])\n\n# 分析这些推文中的情绪\n#从这个地址将包含正面和负面情绪词汇的文本包下载到本地 http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\npos <- scan('positive-words.txt',what='character',comment.char=';')\nneg <- scan('negative-words.txt',what='character',comment.char=';')\n\n# score.sentiment 函数\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\n{\n  require(plyr)\n  require(stringr)\n  \n  # we got a vector of sentences. plyr will handle a list\n  # or a vector as an \"l\" for us\n  # we want a simple array of scores back, so we use \n  # \"l\" + \"a\" + \"ply\" = \"laply\":\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\n    \n    # clean up sentences with R's regex-driven global substitute, gsub():\n    sentence = gsub('[[:punct:]]', '', sentence)\n    sentence = gsub('[[:cntrl:]]', '', sentence)\n    sentence = gsub('\\\\d+', '', sentence)\n    # and convert to lower case:\n    sentence = tolower(sentence)\n    \n    # split into words. str_split is in the stringr package\n    word.list = str_split(sentence, '\\\\s+')\n    # sometimes a list() is one level of hierarchy too much\n    words = unlist(word.list)\n    \n    # compare our words to the dictionaries of positive & negative terms\n    pos.matches = match(words, pos.words)\n    neg.matches = match(words, neg.words)\n    \n    # match() returns the position of the matched term or NA\n    # we just want a TRUE/FALSE:\n    pos.matches = !is.na(pos.matches)\n    neg.matches = !is.na(neg.matches)\n    \n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\n    score = sum(pos.matches) - sum(neg.matches)\n    \n    return(score)\n  }, pos.words, neg.words, .progress=.progress )\n  \n  scores.df = data.frame(score=scores, text=sentences)\n  return(scores.df)\n}\n#最后利用score.sentiment函数将推文与情绪文本进行比对，结果存于result变量中，其中score保存着各推文出现代表不同情绪的词频数。score为正表示正面情绪，为负表示负面情绪。\nresult <- score.sentiment(text,pos,neg)\nsentscore <- result$score\nscoretab <- as.data.frame(table(factor(sentscore)))\n\np <- ggplot(scoretab,aes(y=Freq,x=Var1))\np+geom_bar(stat='identity',aes(fill=Freq))+\n  labs(x='推文情绪值',y='频数')+\n  theme(legend.position='top')\nggsave('6.png')\n\n\n# 分析某个用户的发推内容，提取数据并统计（PM2.5） ----------------------------------------------\n\nlibrary(plyr)\n# 抓取北京和上海空气数据的推文\nairb <- userTimeline(\"beijingair\", n=1000)\nairs <- userTimeline(\"CGShanghaiAir\", n=1000)\n\n#提取文本后用正则表达式分割\npattern  <- '; | \\\\(|\\\\)'\nextract <- function(x) {\n  strsplit(x$text,pattern)\n}\ntextb <- sapply(airb,extract)\ntexts <- sapply(airs,extract)\n\n#转成数据框格式\ndatab <-ldply(textb,.fun=function(x) as.data.frame(t(x),stringsAsFactors=F))\ndatab <- datab[,c(1,4,5)]\ndatas <-ldply(texts,.fun=function(x) as.data.frame(t(x),stringsAsFactors=F))\ndatas <- datas[,c(1,4,5)]\n\n# 合并数据，并转换AQI为数值\ndata <- rbind(datab,datas)\nnames(data) <- c('time','AQI','type')\ndata$AQI <- as.numeric(data$AQI)\n\n# 加入城市变量\ncity <- factor(rep(1:2,each=1000),labels = c('北京','上海'))\ndata$city <- city\n\n# 加入星期变量\ntime.date <-as.Date(data$time,\"%m-%d-%Y\")\ndata$week = as.POSIXlt(time.date)$wday\n\n# 加入钟点变量\ndata$clock <- factor(substr(data$time,start=12,stop=13))\n\n\n# 小提琴图观察不同城市的空气质量\np1 <- ggplot(data,aes(city,AQI,fill=city))\np1 + geom_violin(alpha=0.3,width=0.3) +\n  geom_jitter(alpha=0.3,shape=21) +\n  geom_hline(y=c(100,200,300),color='grey20',linetype=2)+\n  theme(legend.position='none')\nggsave('3.png')\n\n# 上海和北京AQI的分布比较?\np <- ggplot(data,aes(x=AQI,group=city))\np + geom_histogram(aes(fill=city,y=..density..),\n                   alpha=0.5,color='black')+\n  stat_density(geom='line',position='identity',\n               size=0.9,alpha=0.5,color='black')+\n  scale_fill_brewer(palette='Set3')+\n  facet_wrap(~city,ncol=1)+\n  theme(legend.position='none')\nggsave('4.png')\n\n\n# 观察一周的不同日子空气质量中位数\n# aggregate(data$AQI,list(data$week),mean,na.rm=T)\np3 <- ggplot(data,aes(factor(week),AQI,colour=city,group=city))\np3 +stat_summary(fun.y = median, geom='line',size=1.2,aes(linetype=city))+\n  stat_summary(fun.y = median, geom='point',size=4,aes(shape=city))+\n  coord_cartesian(xlim=c(1,7)) +\n  theme_bw() +\n  theme(legend.position=c(0.9,0.9))+\n  labs(x='星期', y='AQI')\nggsave('5.png')\n\n# 观察不同时点空气质量中位数\np2 <- ggplot(data,aes(clock,AQI,colour=city,group=city))\np2 +stat_summary(fun.y = median, geom='line',size=1.2) +\n  stat_summary(fun.y = median, geom='point',size=3)+\n  coord_cartesian(xlim=c(3,26))\n\n\n# 对某个用户的研究 ----------------------------------------------------------------\n#获取用户信息\nmyid <- getUser('xccds')\nmyid$name\nmyid$lastStatus\nmyid$description\nmyid$statusesCount\nmyid$followersCount\nmyid$friendsCount\nmyid$created\n\n# 研究某人lady.gaga的粉丝特点 ---------------------------------------------------------------\n\n\nladygaga <- getUser('ladygaga')\nfollow.lady <- ladygaga$getFollowers(n=5000)\ndf.lady <- do.call('rbind',lapply(follow.lady,as.data.frame))\np <- ggplot(data=df.lady,aes(x=friendsCount,y=followersCount))\np + geom_point(aes(size=statusesCount),color='red4',alpha=0.5)+\n  scale_x_log10(breaks=c(10,100,1000,5000))+\n  scale_y_log10(breaks=c(10,100,1000,1000,20000))+\n  scale_size('发推数',range=c(1,12))+\n  labs(x='关注对象',y='粉丝数')+\n  theme_bw()+\n  theme(legend.position=c(0.9,0.2))\nggsave('8.png')\n\ndf.lady$time <- as.Date(df.lady$created)\ndf.lady$ntime <- as.numeric(df.lady$time)\n\nlibrary(mgcv)\nmodel <- gam(followersCount~s(friendsCount)+s(statusesCount),data=df.lady)\npar(mfrow=c(1,2))\nplot(model,se=T)\n\n# 研究推特趋势 ------------------------------------------------------------------\n\nyesterdayTrends <- getTrends('daily', date=as.character(Sys.Date()-1))\nyesterdayTrends\n\n\n# 研究上推设备 ------------------------------------------------------------------\n\nsources <- sapply(moyanTweets, function(x) x$getStatusSource())\nsources <- str_extract(sources, \"&gt;.+&lt\")\nsources <- str_replace_all(sources, \"&gt;|&lt\", \"\")\nsources <- str_replace(sources,'BlackBerry.+','Blackberry')\n\ncounts <- as.data.frame(table(sources))\ncounts <- subset(counts,Freq>20)\ncounts$sources <- factor(counts$sources)\n#载入ggplot2包，绘制条形图。\np <- ggplot(counts,aes(y=Freq,x=sources))\n\np + geom_bar(aes(fill=Freq),color='black',stat='identity') +\n  scale_x_discrete(limits = levels(counts$sources)[order(counts$Freq)])+\n  geom_text(aes(y=Freq+40,label=paste(Freq/10,'%',sep='')))+\n  coord_flip() + theme(legend.position='none')+\n  labs(y='频数',x='设备')\nggsave('7.png')\n\n# 收集xccds的一千条推文\nlibrary(twitteR)\nxccds <- userTimeline(\"xccds\", n=1000)\nxccds[[1]]$getCreated()\n\nextracttime <- function(x) {\n  return(x$getCreated())\n}\nxccds.time <- lapply(xccds,extracttime)\n\n\nlibrary(lubridate)\n# 转成本时区\ntimefunc <- function(x) {\n  return(with_tz(x,tzone='asia/shanghai'))\n}\nxtime <- ldply(xccds.time, .fun=timefunc)\nxtime$hour <- factor(hour(xtime$V1))\nxtime$week <- factor(wday(xtime$V1))\nxtimedf <- as.data.frame(table(xtime$hour))\n\np <- ggplot(xtimedf,aes(x=Var1,y=Freq))\np+geom_bar(stat='identity',aes(fill=Freq))+\n  theme(legend.position='none')+\n  labs(x='时刻',y='频数')\nggsave('10.png')\n",
    "created" : 1379254218219.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1816620764",
    "id" : "80597931",
    "lastKnownWriteTime" : 1379260245,
    "path" : "D:/DEV/WorkSpaces/Programming-R/kuaibao/demo.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}